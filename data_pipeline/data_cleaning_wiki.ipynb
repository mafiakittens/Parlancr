{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/alexander_mpa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# The future\n",
    "#from __future__ import print_function, division, absolute_import\n",
    "\n",
    "# Data wrangling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as glob\n",
    "import pickle as pickle\n",
    "\n",
    "# Numpy shorthand stuff\n",
    "from numpy import array\n",
    "\n",
    "# NLTK shorthand stuff\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "\n",
    "# SK-learn library for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed some functions from the w266 utils.py file\n",
    "# Miscellaneous helpers\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "        #word = re.sub(r\"(DG)+\", \"DG\", word)\n",
    "    return word\n",
    "\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    #word = re.sub(r\":\",\"\",word)\n",
    "    #word = re.sub(r\"https?\",\"\",word)\n",
    "    #word = re.sub(r\"\\/\",\"\",word)\n",
    "    #word = re.sub(r\"@\",\"\",word)\n",
    "    #word = re.sub(r\"/\\U0001.?'\",\"\",word)\n",
    "    #replace hyperlinks with one instance of \"postedhyperlinkvalue\"\n",
    "    word = re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?\\S*\", \"postedhyperlinkvalue\", word)\n",
    "    word = re.sub(r\"(postedhyperlinkvalue)+\", \"postedhyperlinkvalue\", word)\n",
    "    #only lower case words (2 letters or longer) that are not all upper case\n",
    "    if not word.isupper() or len(word) == 1:\n",
    "        word = word.lower()\n",
    "    #replace things like haha with ha\n",
    "    word = re.sub(r\"([a-z]{2,})\\1{2,}\", r\"\\1\", word)\n",
    "    #replace any three consecutive, identical letters with two instances of that letter\n",
    "    word = re.sub(r\"([a-z])\\1{2,}\", r\"\\1\\1\", word)\n",
    "    #replace any two consecutive, identical consonants at the beginning of a string with one of that consonant\n",
    "    word = re.sub(r\"(^[^aeiou])\\1{1,}\", r\"\\1\", word)\n",
    "    \n",
    "    #replace digits with a stand-in token\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "    \n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "\n",
    "# Made some helper functions of our own\n",
    "from nltk.stem import PorterStemmer   \n",
    "def stem_sentence(token_sent, stemmer=PorterStemmer()):\n",
    "    stem_token_sent = []\n",
    "    for word in token_sent:\n",
    "        stem_token_sent.append(stemmer.stem(word))\n",
    "    return stem_token_sent\n",
    "\n",
    "\n",
    "def sent_plus_word_tokenize(series):\n",
    "    sentences = []\n",
    "    words = []\n",
    "    \n",
    "    for comment in series:\n",
    "        sentences.append(sent_tokenize(comment))\n",
    "    \n",
    "    flat_sentences = [item for sublist in sentences for item in sublist]\n",
    "    \n",
    "    for comment_sentence in flat_sentences:\n",
    "        words.append(word_tokenize(comment_sentence))\n",
    "    \n",
    "    return sentences, words\n",
    "\n",
    "\n",
    "def make_data(data, target='', commentfield='', tokenizer=TweetTokenizer(), canonize=True, stem=True):      \n",
    "    # Separate comments\n",
    "    comments = data.loc[:, commentfield]\n",
    "    #comments = data.loc[:, 'comment_body']\n",
    "    #labels = data.loc[:, target]\n",
    "    \n",
    "    # Convert to list\n",
    "    comment_list = comments.values.tolist()\n",
    "    \n",
    "    # Tokenize comments\n",
    "    tokenizer = tokenizer\n",
    "    # A list of lists of tokenized sentences: word == string/token; sentence == list of string/tokens\n",
    "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in comment_list]\n",
    "    #tokenized_sentences_x = [tokenizer.tokenize(sentence) for sentence in comment_list]\n",
    "    #tokenized_sentences = []\n",
    "    #sentence = []\n",
    "    #last_tok = ''\n",
    "    #for comment in tokenized_sentences_x:\n",
    "    #    for tok in comment:\n",
    "    #        if last_tok in ('http', 'https',':','http:','https:','@'):\n",
    "    #            tok = last_tok + tok\n",
    "    #        if tok in ('http', 'https',':', '@'):\n",
    "    #            last_tok = tok\n",
    "    #        else:\n",
    "    #            last_tok = ''\n",
    "    #            sentence.append(tok)\n",
    "    #    tokenized_sentences.append(sentence)\n",
    "    \n",
    "    if stem:\n",
    "        # Stem words\n",
    "        comments_stem = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            x_tokens_stem = stem_sentence(token_sent=sentence, stemmer=PorterStemmer())\n",
    "            comments_stem.append(x_tokens_stem)\n",
    "        tokenized_sentences = comments_stem\n",
    "    \n",
    "    if canonize:\n",
    "        # Canonize words\n",
    "        comments_canon = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            x_tokens_canon = canonicalize_words(sentence)\n",
    "            comments_canon.append(x_tokens_canon)\n",
    "        # A list of lists of scrubbed tokens; token == word, list == sentence\n",
    "        tokenized_sentences = comments_canon\n",
    "    \n",
    "    x_tokens = tokenized_sentences  \n",
    "    #return comments, x_tokens, labels\n",
    "    return comments, x_tokens\n",
    "\n",
    "\n",
    "def rawlist_to_xtokens(rawlist=['default arg'], vocab_list=[]):\n",
    "    xtokens = []\n",
    "    for rawstring in rawlist:\n",
    "        xtoken = list(filter(lambda x: x in vocab_list, rawstring.split()))\n",
    "        xtokens.append(xtoken)   \n",
    "    return xtokens\n",
    "\n",
    "\n",
    "def xtoken_to_raw(xtoken=['default','arg']):  \n",
    "    raw_text_string = ' '.join(xtoken)\n",
    "    return raw_text_string\n",
    "\n",
    "\n",
    "def raw_to_xtoken(raw_string='default arg'):\n",
    "    xtoken = raw_string.split()\n",
    "    return xtoken\n",
    "\n",
    "\n",
    "def model_diagnostics(model, data, labels, target_names, random=False, test_size=0.10, random_state=42):\n",
    "    \n",
    "    # Split into test and train\n",
    "    # Designate random test_size% of data (rounded up to next obs) as test data\n",
    "    if random:\n",
    "        train_data, test_data, train_labels, test_labels = train_test_split(data, labels, \n",
    "                                                                            test_size=test_size, \n",
    "                                                                            random_state=random_state)\n",
    "    # Designate last test_size% of data (rounded up to next obs) as test data \n",
    "    else:\n",
    "        idx = round(test_size*len(data))\n",
    "        test_data = data[-idx:]\n",
    "        test_labels = labels[-idx:]\n",
    "    \n",
    "    \n",
    "    pred_labels = model.predict_classes(test_data)\n",
    "    \n",
    "    print(\"Test data length is: \", len(test_data))\n",
    "    print(\"Test label length is: \", len(test_labels))\n",
    "    print(\"Pred label length is: \", len(pred_labels))\n",
    "    \n",
    "    confusionMatrix = metrics.confusion_matrix(test_labels, pred_labels)\n",
    "    classificationReport = classification_report(test_labels, pred_labels, target_names=target_names)\n",
    "    \n",
    "    return confusionMatrix, classificationReport\n",
    "\n",
    "\n",
    "# Function to aggregate all of the comments for a given subreddit(s)\n",
    "\n",
    "# Data location example: reddit data for March 2018 downloaded to ~/parlancr/data/reddit/2018_03/\n",
    "# File names: reddit_2018_03000000000000.csv - reddit_2018_03000000000047.csv\n",
    "\n",
    "def export_subreddits(subs):\n",
    "    \n",
    "    selected_subreddits = pd.DataFrame()\n",
    "    file_stem = './data/reddit/*/reddit_*.csv'\n",
    "    \n",
    "    for f in sorted(glob.glob(file_stem)):\n",
    "        \n",
    "        print('Loading comments from: ', f)\n",
    "        partition_comments = pd.read_csv(f)\n",
    "        selected_subreddits = selected_subreddits.append(partition_comments[partition_comments['subreddit'].isin(subs)], ignore_index = True)\n",
    "        \n",
    "    return selected_subreddits\n",
    "\n",
    "\n",
    "def build_model_input(pandas_df, commentfield, post_length, sent_length, tokenizer=TweetTokenizer()):\n",
    "    \n",
    "    sentences, tokens = sent_plus_word_tokenize(pandas_df[commentfield].dropna().values)\n",
    "    \n",
    "    sents = [sents for line in sentences for sents in line if len(line) <= post_length]\n",
    "    \n",
    "    sents_pd = pd.DataFrame({commentfield:sents})\n",
    "    \n",
    "    comments, x_tokens = make_data(sents_pd, commentfield=commentfield, canonize=True, stem=False, tokenizer=tokenizer)\n",
    "    \n",
    "    tokens = [sent for sent in x_tokens if len(sent) <= sent_length]\n",
    "    \n",
    "    raw_list = list(map(xtoken_to_raw, tokens))\n",
    "    \n",
    "    pd_final = pd.DataFrame({commentfield:raw_list})\n",
    "    \n",
    "    #train, validate, test = np.split(pd_final.sample(frac=1), [int(.6*len(pd_final)), int(.8*len(pd_final))])\n",
    "    \n",
    "    #return train, validate, test, pd_final\n",
    "    return pd_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  article_title  paragraph_number  \\\n",
      "0         April                 0   \n",
      "1         April                 0   \n",
      "2         April                 0   \n",
      "3         April                 0   \n",
      "4         April                 0   \n",
      "\n",
      "                                       sentence_text  \n",
      "0  April is the fourth month of the year in the G...  \n",
      "1  April was originally the second month of the R...  \n",
      "2  It became the fourth month of the calendar yea...  \n",
      "3  The derivation of the name -LRB- Latin Aprilis...  \n",
      "4  The traditional etymology is from the Latin ap...  \n",
      "article_title       3851440\n",
      "paragraph_number    3851440\n",
      "sentence_text       3851440\n",
      "dtype: int64\n",
      "  article_title  paragraph_number  \\\n",
      "0         April                 0   \n",
      "1         April                 0   \n",
      "2         April                 0   \n",
      "3         April                 0   \n",
      "4         April                 0   \n",
      "\n",
      "                                       sentence_text  \n",
      "0            April is the fourth month of the year .  \n",
      "1                                   It has 30 days .  \n",
      "2  The name April comes from that Latin word aper...  \n",
      "3  This probably refers to growing plants in spri...  \n",
      "4  April begins on the same day of week as July i...  \n",
      "article_title       505254\n",
      "paragraph_number    505254\n",
      "sentence_text       505254\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wiki_normal = pd.read_csv('data/normal.txt', sep='\\t', header = None, names = ['article_title', 'paragraph_number', 'sentence_text']).dropna(subset=['sentence_text'])\n",
    "wiki_simple = pd.read_csv('data/simple.txt', sep='\\t', header = None, names = ['article_title', 'paragraph_number', 'sentence_text']).dropna(subset=['sentence_text'])\n",
    "\n",
    "print(wiki_normal.head())\n",
    "print(wiki_normal.count())\n",
    "print(wiki_simple.head())\n",
    "print(wiki_simple.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   article_title  paragraph_number  \\\n",
      "29           Art                 0   \n",
      "30           Art                 0   \n",
      "31           Art                 0   \n",
      "32           Art                 1   \n",
      "33           Art                 1   \n",
      "\n",
      "                                        sentence_text  \n",
      "29  Art is the product or process of deliberately ...  \n",
      "30  It encompasses a diverse range of human activi...  \n",
      "31  The meaning of art is explored in a branch of ...  \n",
      "32  Traditionally , the term art was used to refer...  \n",
      "33  This conception changed during the Romantic pe...  \n",
      "article_title       1929002\n",
      "paragraph_number    1929002\n",
      "sentence_text       1929002\n",
      "dtype: int64\n",
      "   article_title  paragraph_number  \\\n",
      "22           Art                 0   \n",
      "23           Art                 0   \n",
      "24           Art                 0   \n",
      "25           Art                 0   \n",
      "26           Art                 0   \n",
      "\n",
      "                                        sentence_text  \n",
      "22  The word art is used to describe some activiti...  \n",
      "23  Therefore , art is made when a human expresses...  \n",
      "24  Some art is useful in a practical sense , such...  \n",
      "25        Many people disagree on how to define art .  \n",
      "26  Many people say people are driven to make art ...  \n",
      "article_title       253761\n",
      "paragraph_number    253761\n",
      "sentence_text       253761\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sampled_articles = np.random.choice(wiki_normal['article_title'].unique(), size = 30000, replace = False)\n",
    "\n",
    "wiki_normal_sampled = wiki_normal.loc[wiki_normal['article_title'].isin(sampled_articles)]\n",
    "wiki_simple_sampled = wiki_simple.loc[wiki_simple['article_title'].isin(sampled_articles)]\n",
    "\n",
    "print(wiki_normal_sampled.head())\n",
    "print(wiki_normal_sampled.count())\n",
    "print(wiki_simple_sampled.head())\n",
    "print(wiki_simple_sampled.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       sentence_text\n",
      "0  art is the product or process of deliberately ...\n",
      "1  it encompasses a diverse range of human activi...\n",
      "2  the meaning of art is explored in a branch of ...\n",
      "3  traditionally , the term art was used to refer...\n",
      "4  this conception changed during the romantic pe...\n",
      "sentence_text    1969307\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "wiki_normal_model_input = build_model_input(pandas_df=wiki_normal_sampled, commentfield='sentence_text', post_length=100, sent_length=1000)\n",
    "\n",
    "print(wiki_normal_model_input.head())\n",
    "print(wiki_normal_model_input.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       sentence_text\n",
      "0  the word art is used to describe some activiti...\n",
      "1  therefore , art is made when a human expresses...\n",
      "2  some art is useful in a practical sense , such...\n",
      "3        many people disagree on how to define art .\n",
      "4  many people say people are driven to make art ...\n",
      "sentence_text    257454\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "wiki_simple_model_input = build_model_input(pandas_df=wiki_simple_sampled, commentfield='sentence_text', post_length=100, sent_length=1000)\n",
    "\n",
    "print(wiki_simple_model_input.head())\n",
    "print(wiki_simple_model_input.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_normal_model_input.to_csv('data/model_data/wiki_normal.train.0', sep='\\t', index=False, header=False)\n",
    "wiki_simple_model_input.to_csv('data/model_data/wiki_simple.train.0', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       sentence_text\n",
      "0                                   He died in 999 .\n",
      "1  Gingerbread was brought to Europe in 992 by th...\n",
      "2  It was the custom to bake white biscuits and p...\n",
      "3  During the 13th century , gingerbread was brou...\n",
      "4  It then referred to a confection made with hon...\n",
      "sentence_text    284677\n",
      "dtype: int64\n",
      "                                       sentence_text\n",
      "0                                   He died in 999 .\n",
      "1  Armenian monk Gregory of Nicopolis ( Gregory M...\n",
      "2  The custom was to bake white biscuits and pain...\n",
      "3  German immigrants brought it to Sweden during ...\n",
      "4  After , it was a confection made with honey an...\n",
      "sentence_text    284677\n",
      "dtype: int64\n",
      "                                       sentence_text\n",
      "0                                    he died in DG .\n",
      "1  gingerbread was brought to europe in DGDG by t...\n",
      "2  it was the custom to bake white biscuits and p...\n",
      "3  during the 13th century , gingerbread was brou...\n",
      "4  it then referred to a confection made with hon...\n",
      "sentence_text    289069\n",
      "dtype: int64\n",
      "                                       sentence_text\n",
      "0                                    he died in DG .\n",
      "1  armenian monk gregory of nicopolis ( gregory m...\n",
      "2  the custom was to bake white biscuits and pain...\n",
      "3  german immigrants brought it to sweden during ...\n",
      "4  after , it was a confection made with honey an...\n",
      "sentence_text    287536\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Secondary set of wikipedia data, filtered to well-matched sentences (https://github.com/senisioi/NeuralTextSimplification)\n",
    "\n",
    "wiki2_normal = pd.read_csv('data/train_normal_matched.txt', sep='\\t', header = None, names = ['sentence_text']).dropna(subset=['sentence_text'])\n",
    "wiki2_simple = pd.read_csv('data/train_simple_matched.txt', sep='\\t', header = None, names = ['sentence_text']).dropna(subset=['sentence_text'])\n",
    "\n",
    "print(wiki2_normal.head())\n",
    "print(wiki2_normal.count())\n",
    "print(wiki2_simple.head())\n",
    "print(wiki2_simple.count())\n",
    "\n",
    "wiki2_normal_model_input = build_model_input(pandas_df=wiki2_normal, commentfield='sentence_text', post_length=100, sent_length=1000)\n",
    "\n",
    "print(wiki2_normal_model_input.head())\n",
    "print(wiki2_normal_model_input.count())\n",
    "\n",
    "wiki2_simple_model_input = build_model_input(pandas_df=wiki2_simple, commentfield='sentence_text', post_length=100, sent_length=1000)\n",
    "\n",
    "print(wiki2_simple_model_input.head())\n",
    "print(wiki2_simple_model_input.count())\n",
    "\n",
    "wiki2_normal_model_input.to_csv('data/model_data/wiki_normal_matched.train.0', sep='\\t', index=False, header=False)\n",
    "wiki2_simple_model_input.to_csv('data/model_data/wiki_simple_matched.train.0', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
