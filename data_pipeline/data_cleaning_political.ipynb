{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/alexander_mpa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/alexander_mpa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# The future\n",
    "#from __future__ import print_function, division, absolute_import\n",
    "\n",
    "# Data wrangling libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from io import StringIO\n",
    "import html\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as glob\n",
    "import pickle as pickle\n",
    "\n",
    "# Numpy shorthand stuff\n",
    "from numpy import array\n",
    "\n",
    "# NLTK shorthand stuff\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import TweetTokenizer, RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# SK-learn library for splitting data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Borrowed some functions from the w266 utils.py file\n",
    "# Miscellaneous helpers\n",
    "def flatten(list_of_lists):\n",
    "    \"\"\"Flatten a list-of-lists into a single list.\"\"\"\n",
    "    return list(itertools.chain.from_iterable(list_of_lists))\n",
    "\n",
    "\n",
    "# Word processing functions\n",
    "def canonicalize_digits(word):\n",
    "    if any([c.isalpha() for c in word]): return word\n",
    "    word = re.sub(\"\\d\", \"DG\", word)\n",
    "    if word.startswith(\"DG\"):\n",
    "        word = word.replace(\",\", \"\") # remove thousands separator\n",
    "        #word = re.sub(r\"(DG)+\", \"DG\", word)\n",
    "    return word\n",
    "\n",
    "\n",
    "def canonicalize_word(word, wordset=None, digits=True):\n",
    "    #word = re.sub(r\":\",\"\",word)\n",
    "    #word = re.sub(r\"https?\",\"\",word)\n",
    "    #word = re.sub(r\"\\/\",\"\",word)\n",
    "    #word = re.sub(r\"@\",\"\",word)\n",
    "    #word = re.sub(r\"/\\U0001.?'\",\"\",word)\n",
    "    #replace hyperlinks with one instance of \"postedhyperlinkvalue\"\n",
    "    word = re.sub(r\"(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w\\.-]*)*\\/?\\S*\", \"postedhyperlinkvalue\", word)\n",
    "    word = re.sub(r\"(postedhyperlinkvalue)+\", \"postedhyperlinkvalue\", word)\n",
    "    #only lower case words (2 letters or longer) that are not all upper case\n",
    "    if not word.isupper() or len(word) == 1:\n",
    "        word = word.lower()\n",
    "    #replace things like haha with ha\n",
    "    word = re.sub(r\"([a-z]{2,})\\1{2,}\", r\"\\1\", word)\n",
    "    #replace any three consecutive, identical letters with two instances of that letter\n",
    "    word = re.sub(r\"([a-z])\\1{2,}\", r\"\\1\\1\", word)\n",
    "    #replace any two consecutive, identical consonants at the beginning of a string with one of that consonant\n",
    "    word = re.sub(r\"(^[^aeiou])\\1{1,}\", r\"\\1\", word)\n",
    "    \n",
    "    #replace digits with a stand-in token\n",
    "    if digits:\n",
    "        if (wordset != None) and (word in wordset): return word\n",
    "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
    "    if (wordset == None) or (word in wordset):\n",
    "        return word\n",
    "    else:\n",
    "        return constants.UNK_TOKEN\n",
    "\n",
    "    \n",
    "def canonicalize_words(words, **kw):\n",
    "    return [canonicalize_word(word, **kw) for word in words]\n",
    "\n",
    "\n",
    "# Made some helper functions of our own\n",
    "from nltk.stem import PorterStemmer   \n",
    "def stem_sentence(token_sent, stemmer=PorterStemmer()):\n",
    "    stem_token_sent = []\n",
    "    for word in token_sent:\n",
    "        stem_token_sent.append(stemmer.stem(word))\n",
    "    return stem_token_sent\n",
    "\n",
    "\n",
    "def sent_plus_word_tokenize(series):\n",
    "    sentences = []\n",
    "    words = []\n",
    "    \n",
    "    for comment in series:\n",
    "        sentences.append(sent_tokenize(comment))\n",
    "    \n",
    "    flat_sentences = [item for sublist in sentences for item in sublist]\n",
    "    \n",
    "    for comment_sentence in flat_sentences:\n",
    "        words.append(word_tokenize(comment_sentence))\n",
    "    \n",
    "    return sentences, words\n",
    "\n",
    "\n",
    "def make_data(data, target='', commentfield='', tokenizer=TweetTokenizer(), canonize=True, stem=True, nnp=False, htmlDecode=True):      \n",
    "    # Separate comments\n",
    "    comments = data.loc[:, commentfield]\n",
    "    #comments = data.loc[:, 'comment_body']\n",
    "    #labels = data.loc[:, target]\n",
    "    \n",
    "    # Convert to list\n",
    "    comment_list = comments.values.tolist()\n",
    "    \n",
    "    if htmlDecode:\n",
    "        comment_list_html_decoded = []\n",
    "        for i in comment_list:\n",
    "            comment_list_html_decoded.append(html.unescape(i))\n",
    "        comment_list = comment_list_html_decoded\n",
    "    \n",
    "    \n",
    "    # Tokenize comments\n",
    "    tokenizer = tokenizer\n",
    "    # A list of lists of tokenized sentences: word == string/token; sentence == list of string/tokens\n",
    "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in comment_list]\n",
    "    #tokenized_sentences_x = [tokenizer.tokenize(sentence) for sentence in comment_list]\n",
    "    #tokenized_sentences = []\n",
    "    #sentence = []\n",
    "    #last_tok = ''\n",
    "    #for comment in tokenized_sentences_x:\n",
    "    #    for tok in comment:\n",
    "    #        if last_tok in ('http', 'https',':','http:','https:','@'):\n",
    "    #            tok = last_tok + tok\n",
    "    #        if tok in ('http', 'https',':', '@'):\n",
    "    #            last_tok = tok\n",
    "    #        else:\n",
    "    #            last_tok = ''\n",
    "    #            sentence.append(tok)\n",
    "    #    tokenized_sentences.append(sentence)\n",
    "    \n",
    "    if nnp:\n",
    "        # Canonize proper nouns using stanford named entity recognizer\n",
    "        comments_nnp = []\n",
    "        \n",
    "        #stanford ner tagger\n",
    "        #st = StanfordNERTagger('../../../stanford_ner/stanford-ner-2018-02-27/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "        #                       '../../../stanford_ner/stanford-ner-2018-02-27/stanford-ner.jar',\n",
    "        #                       encoding='utf-8')\n",
    "        \n",
    "        #nltk pos tagger -> pos_tag()\n",
    "        \n",
    "        for sentence in tokenized_sentences:\n",
    "            #tagged_sent = st.tag(sentence)\n",
    "            tagged_sent = pos_tag(sentence)\n",
    "            cannonized_sent = []\n",
    "            for i in tagged_sent:\n",
    "                #if i[1] in ['PERSON', 'LOCATION', 'ORGANIZATION']:\n",
    "                if i[1] == 'NNP':\n",
    "                    cannonized_sent.append(i[1])\n",
    "                else:\n",
    "                    cannonized_sent.append(i[0])\n",
    "                    \n",
    "            comments_nnp.append(cannonized_sent)\n",
    "        tokenized_sentences = comments_nnp\n",
    "    \n",
    "    if stem:\n",
    "        # Stem words\n",
    "        comments_stem = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            x_tokens_stem = stem_sentence(token_sent=sentence, stemmer=PorterStemmer())\n",
    "            comments_stem.append(x_tokens_stem)\n",
    "        tokenized_sentences = comments_stem\n",
    "    \n",
    "    if canonize:\n",
    "        # Canonize words\n",
    "        comments_canon = []\n",
    "        for sentence in tokenized_sentences:\n",
    "            x_tokens_canon = canonicalize_words(sentence)\n",
    "            comments_canon.append(x_tokens_canon)\n",
    "        tokenized_sentences = comments_canon\n",
    "    \n",
    "    x_tokens = tokenized_sentences  \n",
    "    #return comments, x_tokens, labels\n",
    "    return comments, x_tokens\n",
    "\n",
    "\n",
    "def rawlist_to_xtokens(rawlist=['default arg'], vocab_list=[]):\n",
    "    xtokens = []\n",
    "    for rawstring in rawlist:\n",
    "        xtoken = list(filter(lambda x: x in vocab_list, rawstring.split()))\n",
    "        xtokens.append(xtoken)   \n",
    "    return xtokens\n",
    "\n",
    "\n",
    "def xtoken_to_raw(xtoken=['default','arg']):  \n",
    "    raw_text_string = ' '.join(xtoken)\n",
    "    return raw_text_string\n",
    "\n",
    "\n",
    "def raw_to_xtoken(raw_string='default arg'):\n",
    "    xtoken = raw_string.split()\n",
    "    return xtoken\n",
    "\n",
    "\n",
    "def model_diagnostics(model, data, labels, target_names, random=False, test_size=0.10, random_state=42):\n",
    "    \n",
    "    # Split into test and train\n",
    "    # Designate random test_size% of data (rounded up to next obs) as test data\n",
    "    if random:\n",
    "        train_data, test_data, train_labels, test_labels = train_test_split(data, labels, \n",
    "                                                                            test_size=test_size, \n",
    "                                                                            random_state=random_state)\n",
    "    # Designate last test_size% of data (rounded up to next obs) as test data \n",
    "    else:\n",
    "        idx = round(test_size*len(data))\n",
    "        test_data = data[-idx:]\n",
    "        test_labels = labels[-idx:]\n",
    "    \n",
    "    \n",
    "    pred_labels = model.predict_classes(test_data)\n",
    "    \n",
    "    print(\"Test data length is: \", len(test_data))\n",
    "    print(\"Test label length is: \", len(test_labels))\n",
    "    print(\"Pred label length is: \", len(pred_labels))\n",
    "    \n",
    "    confusionMatrix = metrics.confusion_matrix(test_labels, pred_labels)\n",
    "    classificationReport = classification_report(test_labels, pred_labels, target_names=target_names)\n",
    "    \n",
    "    return confusionMatrix, classificationReport\n",
    "\n",
    "\n",
    "# Function to aggregate all of the comments for a given subreddit(s)\n",
    "\n",
    "# Data location example: reddit data for March 2018 downloaded to ~/parlancr/data/reddit/2018_03/\n",
    "# File names: reddit_2018_03000000000000.csv - reddit_2018_03000000000047.csv\n",
    "\n",
    "def export_subreddits(subs):\n",
    "    \n",
    "    selected_subreddits = pd.DataFrame()\n",
    "    file_stem = './data/reddit/*/reddit_*.csv'\n",
    "    \n",
    "    for f in sorted(glob.glob(file_stem)):\n",
    "        \n",
    "        print('Loading comments from: ', f)\n",
    "        partition_comments = pd.read_csv(f)\n",
    "        selected_subreddits = selected_subreddits.append(partition_comments[partition_comments['subreddit'].isin(subs)], ignore_index = True)\n",
    "        \n",
    "    return selected_subreddits\n",
    "\n",
    "\n",
    "def build_model_input(pandas_df, commentfield, post_length, sent_length, tokenizer=TweetTokenizer()):\n",
    "    \n",
    "    comments, x_tokens = make_data(pandas_df, commentfield=commentfield, canonize=True, stem=False, tokenizer=tokenizer)\n",
    "    \n",
    "    tokens = [sent for sent in x_tokens if len(sent) <= sent_length]\n",
    "    \n",
    "    raw_list = list(map(xtoken_to_raw, tokens))\n",
    "    \n",
    "    pd_final = pd.DataFrame({commentfield:raw_list})\n",
    "    \n",
    "    return pd_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data & Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_text    268961\n",
      "dtype: int64\n",
      "sentence_text    268961\n",
      "dtype: int64\n",
      "sentence_text    268961\n",
      "dtype: int64\n",
      "sentence_text    268961\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# train set with html decoding\n",
    "political_democrat = pd.read_csv('data/twitter/politcal_tweets/political_data/democratic_only.train.en', sep='\\t', header = None, names = ['sentence_text']).dropna(subset=['sentence_text'])\n",
    "political_republican = pd.read_csv('data/twitter/politcal_tweets/political_data/republican_only.train.en', sep='\\t', header = None, names = ['sentence_text']).dropna(subset=['sentence_text'])\n",
    "\n",
    "print(political_democrat.count())\n",
    "print(political_republican.count())\n",
    "\n",
    "political_democrat_model_input = build_model_input(pandas_df=political_democrat, commentfield='sentence_text', post_length=1000, sent_length=1000)\n",
    "\n",
    "print(political_democrat_model_input.count())\n",
    "\n",
    "political_republican_model_input = build_model_input(pandas_df=political_republican, commentfield='sentence_text', post_length=1000, sent_length=1000)\n",
    "\n",
    "print(political_republican_model_input.count())\n",
    "\n",
    "political_democrat_model_input.to_csv('data/model_data/political_democrat_train.txt', sep='\\t', index=False, header=False)\n",
    "political_republican_model_input.to_csv('data/model_data/political_republican_train.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_text    2000\n",
      "dtype: int64\n",
      "sentence_text    2000\n",
      "dtype: int64\n",
      "sentence_text    2000\n",
      "dtype: int64\n",
      "sentence_text    2000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dev set with html decoding\n",
    "political_democrat_dev = pd.read_csv('data/twitter/politcal_tweets/political_data/democratic_only.dev.en', sep='\\t', header = None, names = ['sentence_text']).dropna(subset=['sentence_text'])\n",
    "political_republican_dev = pd.read_csv('data/twitter/politcal_tweets/political_data/republican_only.dev.en', sep='\\t', header = None, names = ['sentence_text']).dropna(subset=['sentence_text'])\n",
    "\n",
    "print(political_democrat_dev.count())\n",
    "print(political_republican_dev.count())\n",
    "\n",
    "political_democrat_model_input_dev = build_model_input(pandas_df=political_democrat_dev, commentfield='sentence_text', post_length=1000, sent_length=1000)\n",
    "\n",
    "print(political_democrat_model_input_dev.count())\n",
    "\n",
    "political_republican_model_input_dev = build_model_input(pandas_df=political_republican_dev, commentfield='sentence_text', post_length=1000, sent_length=1000)\n",
    "\n",
    "print(political_republican_model_input_dev.count())\n",
    "\n",
    "political_democrat_model_input_dev.to_csv('data/model_data/political_democrat_dev.txt', sep='\\t', index=False, header=False)\n",
    "political_republican_model_input_dev.to_csv('data/model_data/political_republican_dev.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_text    28000\n",
      "dtype: int64\n",
      "sentence_text    28000\n",
      "dtype: int64\n",
      "sentence_text    28000\n",
      "dtype: int64\n",
      "sentence_text    28000\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# test set with html decoding\n",
    "political_democrat_test = pd.read_csv('data/twitter/politcal_tweets/political_data/democratic_only.test.en', sep='\\t', header = None, names = ['sentence_text']).dropna(subset=['sentence_text'])\n",
    "political_republican_test = pd.read_csv('data/twitter/politcal_tweets/political_data/republican_only.test.en', sep='\\t', header = None, names = ['sentence_text']).dropna(subset=['sentence_text'])\n",
    "\n",
    "print(political_democrat_test.count())\n",
    "print(political_republican_test.count())\n",
    "\n",
    "political_democrat_model_input_test = build_model_input(pandas_df=political_democrat_test, commentfield='sentence_text', post_length=1000, sent_length=1000)\n",
    "\n",
    "print(political_democrat_model_input_test.count())\n",
    "\n",
    "political_republican_model_input_test = build_model_input(pandas_df=political_republican_test, commentfield='sentence_text', post_length=1000, sent_length=1000)\n",
    "\n",
    "print(political_republican_model_input_test.count())\n",
    "\n",
    "political_democrat_model_input_test.to_csv('data/model_data/political_democrat_test.txt', sep='\\t', index=False, header=False)\n",
    "political_republican_model_input_test.to_csv('data/model_data/political_republican_test.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
